{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trend Detection: Temperature vs Market Probability Changes\n",
    "\n",
    "This notebook implements trend detection algorithms to analyze the relationship between temperature changes and Polymarket probability shifts. We'll explore:\n",
    "\n",
    "1. **Temperature Trend Detection**: Using statistical methods to identify temperature trends\n",
    "2. **Market Probability Analysis**: Analyzing probability changes in weather-related markets\n",
    "3. **Correlation Analysis**: Statistical correlation between temperature trends and market movements\n",
    "4. **Visualization**: Interactive plots showing trend relationships\n",
    "\n",
    "## Data Sources\n",
    "- **Weather Data**: Historical temperature data from Meteostat, Met Office, NWS\n",
    "- **Market Data**: Polymarket probability data for weather-related events\n",
    "- **Database**: SQLite database with normalized weather and market data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical and ML libraries\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection and data loading\n",
    "class ClimateTradeAnalyzer:\n",
    "    \"\"\"Main class for analyzing climate-weather market correlations\"\"\"\n",
    "    \n",
    "    def __init__(self, db_path=\"../data/climatetrade.db\"):\n",
    "        self.db_path = db_path\n",
    "        self.weather_data = None\n",
    "        self.market_data = None\n",
    "        \n",
    "    def load_weather_data(self, location=None, start_date=None, end_date=None):\n",
    "        \"\"\"Load weather data from database\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        \n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            timestamp,\n",
    "            location_name,\n",
    "            temperature,\n",
    "            temperature_min,\n",
    "            temperature_max,\n",
    "            humidity,\n",
    "            wind_speed,\n",
    "            precipitation,\n",
    "            pressure\n",
    "        FROM weather_data\n",
    "        WHERE temperature IS NOT NULL\n",
    "        \"\"\"\n",
    "        \n",
    "        if location:\n",
    "            query += f\" AND location_name LIKE '%{location}%'\"\n",
    "        if start_date:\n",
    "            query += f\" AND timestamp >= '{start_date}'\"\n",
    "        if end_date:\n",
    "            query += f\" AND timestamp <= '{end_date}'\"\n",
    "            \n",
    "        query += \" ORDER BY timestamp\"\n",
    "        \n",
    "        self.weather_data = pd.read_sql_query(query, conn)\n",
    "        self.weather_data['timestamp'] = pd.to_datetime(self.weather_data['timestamp'])\n",
    "        self.weather_data.set_index('timestamp', inplace=True)\n",
    "        \n",
    "        conn.close()\n",
    "        print(f\"Loaded {len(self.weather_data)} weather records\")\n",
    "        return self.weather_data\n",
    "    \n",
    "    def load_market_data(self, event_filter=None, start_date=None, end_date=None):\n",
    "        \"\"\"Load market data from database\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        \n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            timestamp,\n",
    "            event_title,\n",
    "            market_id,\n",
    "            outcome_name,\n",
    "            probability,\n",
    "            volume\n",
    "        FROM polymarket_data\n",
    "        WHERE probability IS NOT NULL\n",
    "        \"\"\"\n",
    "        \n",
    "        if event_filter:\n",
    "            query += f\" AND event_title LIKE '%{event_filter}%'\"\n",
    "        if start_date:\n",
    "            query += f\" AND timestamp >= '{start_date}'\"\n",
    "        if end_date:\n",
    "            query += f\" AND timestamp <= '{end_date}'\"\n",
    "            \n",
    "        query += \" ORDER BY timestamp\"\n",
    "        \n",
    "        self.market_data = pd.read_sql_query(query, conn)\n",
    "        self.market_data['timestamp'] = pd.to_datetime(self.market_data['timestamp'])\n",
    "        self.market_data.set_index('timestamp', inplace=True)\n",
    "        \n",
    "        conn.close()\n",
    "        print(f\"Loaded {len(self.market_data)} market records\")\n",
    "        return self.market_data\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = ClimateTradeAnalyzer()\n",
    "print(\"ClimateTradeAnalyzer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data for analysis\n",
    "# Note: Adjust dates and locations based on your available data\n",
    "\n",
    "# Load weather data (London example)\n",
    "weather_df = analyzer.load_weather_data(\n",
    "    location=\"London\",\n",
    "    start_date=\"2020-01-01\",\n",
    "    end_date=\"2024-12-31\"\n",
    ")\n",
    "\n",
    "# Load market data (weather-related events)\n",
    "market_df = analyzer.load_market_data(\n",
    "    event_filter=\"weather\",\n",
    "    start_date=\"2020-01-01\",\n",
    "    end_date=\"2024-12-31\"\n",
    ")\n",
    "\n",
    "print(\"\\nData Overview:\")\n",
    "print(\"=\" * 50)\n",
    "if weather_df is not None and not weather_df.empty:\n",
    "    print(f\"Weather data shape: {weather_df.shape}\")\n",
    "    print(f\"Weather date range: {weather_df.index.min()} to {weather_df.index.max()}\")\n",
    "    print(f\"Locations: {weather_df['location_name'].unique()}\")\n",
    "\n",
    "if market_df is not None and not market_df.empty:\n",
    "    print(f\"\\nMarket data shape: {market_df.shape}\")\n",
    "    print(f\"Market date range: {market_df.index.min()} to {market_df.index.max()}\")\n",
    "    print(f\"Events: {market_df['event_title'].nunique()} unique events\")\n",
    "    print(f\"Markets: {market_df['market_id'].nunique()} unique markets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Temperature Trend Detection Algorithms\n",
    "\n",
    "We'll implement several trend detection methods:\n",
    "- **Linear Regression Trend**: Simple slope-based trend detection\n",
    "- **Moving Average Crossover**: Short-term vs long-term trend identification\n",
    "- **Seasonal Decomposition**: Extracting trend from seasonal patterns\n",
    "- **Change Point Detection**: Identifying significant trend changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrendDetector:\n",
    "    \"\"\"Class for detecting trends in time series data\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "    \n",
    "    def linear_trend(self, data, window_days=30):\n",
    "        \"\"\"Calculate linear trend using rolling regression\"\"\"\n",
    "        trends = []\n",
    "        \n",
    "        for i in range(window_days, len(data)):\n",
    "            window = data.iloc[i-window_days:i]\n",
    "            if len(window.dropna()) < window_days * 0.8:  # Require 80% data\n",
    "                trends.append(np.nan)\n",
    "                continue\n",
    "                \n",
    "            # Linear regression on window\n",
    "            x = np.arange(len(window))\n",
    "            y = window.values\n",
    "            mask = ~np.isnan(y)\n",
    "            \n",
    "            if np.sum(mask) < 2:\n",
    "                trends.append(np.nan)\n",
    "                continue\n",
    "                \n",
    "            slope, intercept, r_value, p_value, std_err = stats.linregress(x[mask], y[mask])\n",
    "            trends.append(slope)\n",
    "        \n",
    "        # Pad with NaN for initial window\n",
    "        trends = [np.nan] * (window_days - 1) + trends\n",
    "        return pd.Series(trends, index=data.index)\n",
    "    \n",
    "    def moving_average_crossover(self, data, short_window=7, long_window=30):\n",
    "        \"\"\"Detect trend changes using moving average crossover\"\"\"\n",
    "        short_ma = data.rolling(window=short_window, center=True).mean()\n",
    "        long_ma = data.rolling(window=long_window, center=True).mean()\n",
    "        \n",
    "        # Trend signal: 1 for uptrend, -1 for downtrend, 0 for neutral\n",
    "        trend_signal = np.where(short_ma > long_ma, 1, \n",
    "                               np.where(short_ma < long_ma, -1, 0))\n",
    "        \n",
    "        return pd.Series(trend_signal, index=data.index)\n",
    "    \n",
    "    def seasonal_trend(self, data, period=365):\n",
    "        \"\"\"Extract trend component using seasonal decomposition\"\"\"\n",
    "        try:\n",
    "            # Resample to daily if needed\n",
    "            if isinstance(data.index, pd.DatetimeIndex):\n",
    "                daily_data = data.resample('D').mean()\n",
    "            else:\n",
    "                daily_data = data\n",
    "            \n",
    "            # Remove NaN values for decomposition\n",
    "            clean_data = daily_data.dropna()\n",
    "            \n",
    "            if len(clean_data) < period * 2:\n",
    "                print(f\"Warning: Not enough data for seasonal decomposition. Need at least {period * 2} points.\")\n",
    "                return pd.Series([np.nan] * len(data), index=data.index)\n",
    "            \n",
    "            decomposition = seasonal_decompose(clean_data, model='additive', period=period)\n",
    "            trend = decomposition.trend\n",
    "            \n",
    "            # Reindex to original data\n",
    "            trend_reindexed = trend.reindex(data.index, method='nearest')\n",
    "            return trend_reindexed\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in seasonal decomposition: {e}\")\n",
    "            return pd.Series([np.nan] * len(data), index=data.index)\n",
    "    \n",
    "    def detect_change_points(self, data, threshold=2.0):\n",
    "        \"\"\"Detect significant change points in the trend\"\"\"\n",
    "        # Calculate rolling standard deviation\n",
    "        rolling_std = data.rolling(window=30).std()\n",
    "        \n",
    "        # Calculate z-score of changes\n",
    "        diff = data.diff()\n",
    "        z_scores = np.abs((diff - diff.mean()) / diff.std())\n",
    "        \n",
    "        # Identify change points where z-score exceeds threshold\n",
    "        change_points = z_scores > threshold\n",
    "        \n",
    "        return change_points\n",
    "\n",
    "# Initialize trend detector\n",
    "trend_detector = TrendDetector()\n",
    "print(\"TrendDetector initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply trend detection to temperature data\n",
    "if weather_df is not None and not weather_df.empty:\n",
    "    print(\"Applying trend detection algorithms to temperature data...\")\n",
    "    \n",
    "    # Extract temperature series\n",
    "    temp_series = weather_df['temperature'].dropna()\n",
    "    \n",
    "    if len(temp_series) > 0:\n",
    "        # 1. Linear trend detection\n",
    "        linear_trends = trend_detector.linear_trend(temp_series, window_days=30)\n",
    "        \n",
    "        # 2. Moving average crossover\n",
    "        ma_signals = trend_detector.moving_average_crossover(temp_series)\n",
    "        \n",
    "        # 3. Seasonal trend\n",
    "        seasonal_trends = trend_detector.seasonal_trend(temp_series)\n",
    "        \n",
    "        # 4. Change point detection\n",
    "        change_points = trend_detector.detect_change_points(temp_series)\n",
    "        \n",
    "        print(\"Trend detection completed!\")\n",
    "        print(f\"Linear trends calculated: {linear_trends.count()} points\")\n",
    "        print(f\"MA signals: {ma_signals.value_counts().to_dict()}\")\n",
    "        print(f\"Change points detected: {change_points.sum()}\")\n",
    "    else:\n",
    "        print(\"No temperature data available for trend analysis\")\n",
    "else:\n",
    "    print(\"No weather data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temperature trends\n",
    "if 'temp_series' in locals() and len(temp_series) > 0:\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "    \n",
    "    # Plot 1: Original temperature data with trends\n",
    "    axes[0].plot(temp_series.index, temp_series.values, 'b-', alpha=0.7, label='Temperature')\n",
    "    if 'seasonal_trends' in locals():\n",
    "        axes[0].plot(seasonal_trends.index, seasonal_trends.values, 'r-', linewidth=2, label='Seasonal Trend')\n",
    "    axes[0].set_title('Temperature Data with Seasonal Trend')\n",
    "    axes[0].set_ylabel('Temperature (°C)')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Linear trends\n",
    "    if 'linear_trends' in locals():\n",
    "        valid_trends = linear_trends.dropna()\n",
    "        axes[1].plot(valid_trends.index, valid_trends.values, 'g-', label='Linear Trend Slope')\n",
    "        axes[1].axhline(y=0, color='k', linestyle='--', alpha=0.5, label='No Trend')\n",
    "        axes[1].set_title('Linear Trend Detection (30-day windows)')\n",
    "        axes[1].set_ylabel('Trend Slope')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Moving average signals\n",
    "    if 'ma_signals' in locals():\n",
    "        colors = ['red' if x == -1 else 'green' if x == 1 else 'gray' for x in ma_signals.values]\n",
    "        axes[2].scatter(ma_signals.index, ma_signals.values, c=colors, alpha=0.6, s=20)\n",
    "        axes[2].set_title('Moving Average Crossover Signals')\n",
    "        axes[2].set_ylabel('Trend Signal')\n",
    "        axes[2].set_yticks([-1, 0, 1])\n",
    "        axes[2].set_yticklabels(['Downtrend', 'Neutral', 'Uptrend'])\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nTrend Analysis Summary:\")\n",
    "    print(\"=\" * 40)\n",
    "    if 'linear_trends' in locals():\n",
    "        positive_trends = (linear_trends > 0).sum()\n",
    "        negative_trends = (linear_trends < 0).sum()\n",
    "        print(f\"Positive trends: {positive_trends}\")\n",
    "        print(f\"Negative trends: {negative_trends}\")\n",
    "        print(f\"Average trend slope: {linear_trends.mean():.6f}\")\n",
    "    \n",
    "    if 'change_points' in locals():\n",
    "        print(f\"Significant change points: {change_points.sum()}\")\n",
    "else:\n",
    "    print(\"No temperature data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Market Probability Change Analysis\n",
    "\n",
    "Now let's analyze how market probabilities change over time, particularly for weather-related events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze market probability changes\n",
    "if market_df is not None and not market_df.empty:\n",
    "    print(\"Analyzing market probability changes...\")\n",
    "    \n",
    "    # Group by market and calculate probability changes\n",
    "    market_groups = market_df.groupby('market_id')\n",
    "    \n",
    "    probability_changes = []\n",
    "    \n",
    "    for market_id, group in market_groups:\n",
    "        if len(group) > 1:\n",
    "            # Sort by timestamp\n",
    "            group_sorted = group.sort_index()\n",
    "            \n",
    "            # Calculate probability changes\n",
    "            prob_changes = group_sorted['probability'].diff()\n",
    "            \n",
    "            # Calculate volatility (standard deviation of changes)\n",
    "            volatility = prob_changes.std()\n",
    "            \n",
    "            # Calculate trend in probability\n",
    "            prob_trend = trend_detector.linear_trend(group_sorted['probability'], window_days=7)\n",
    "            \n",
    "            probability_changes.append({\n",
    "                'market_id': market_id,\n",
    "                'event_title': group['event_title'].iloc[0],\n",
    "                'data_points': len(group),\n",
    "                'probability_range': group['probability'].max() - group['probability'].min(),\n",
    "                'volatility': volatility,\n",
    "                'avg_probability': group['probability'].mean(),\n",
    "                'final_probability': group_sorted['probability'].iloc[-1],\n",
    "                'probability_trend': prob_trend.mean() if not prob_trend.empty else 0\n",
    "            })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    prob_change_df = pd.DataFrame(probability_changes)\n",
    "    \n",
    "    print(f\"Analyzed {len(probability_changes)} markets\")\n",
    "    print(\"\\nTop 10 most volatile markets:\")\n",
    "    if not prob_change_df.empty:\n",
    "        top_volatile = prob_change_df.nlargest(10, 'volatility')\n",
    "        for _, row in top_volatile.iterrows():\n",
    "            print(f\"{row['market_id'][:8]}...: {row['event_title'][:50]}... (Vol: {row['volatility']:.4f})\")\n",
    "else:\n",
    "    print(\"No market data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Temperature-Market Correlation Analysis\n",
    "\n",
    "Now we'll correlate temperature trends with market probability changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis between temperature and market data\n",
    "def analyze_temperature_market_correlation(weather_df, market_df, max_lag_days=7):\n",
    "    \"\"\"Analyze correlation between temperature trends and market probability changes\"\"\"\n",
    "    \n",
    "    if weather_df is None or market_df is None or weather_df.empty or market_df.empty:\n",
    "        print(\"Insufficient data for correlation analysis\")\n",
    "        return None\n",
    "    \n",
    "    correlations = []\n",
    "    \n",
    "    # Get temperature data\n",
    "    temp_data = weather_df['temperature'].dropna()\n",
    "    \n",
    "    # Analyze each market\n",
    "    for market_id in market_df['market_id'].unique():\n",
    "        market_subset = market_df[market_df['market_id'] == market_id]\n",
    "        prob_data = market_subset['probability'].dropna()\n",
    "        \n",
    "        if len(prob_data) < 10:  # Skip markets with too few data points\n",
    "            continue\n",
    "        \n",
    "        # Find overlapping time period\n",
    "        start_date = max(temp_data.index.min(), prob_data.index.min())\n",
    "        end_date = min(temp_data.index.max(), prob_data.index.max())\n",
    "        \n",
    "        if start_date >= end_date:\n",
    "            continue\n",
    "        \n",
    "        # Align data to common time period\n",
    "        temp_aligned = temp_data[(temp_data.index >= start_date) & (temp_data.index <= end_date)]\n",
    "        prob_aligned = prob_data[(prob_data.index >= start_date) & (prob_data.index <= end_date)]\n",
    "        \n",
    "        if len(temp_aligned) < 10 or len(prob_aligned) < 10:\n",
    "            continue\n",
    "        \n",
    "        # Resample to daily frequency for correlation\n",
    "        temp_daily = temp_aligned.resample('D').mean()\n",
    "        prob_daily = prob_aligned.resample('D').mean()\n",
    "        \n",
    "        # Calculate correlations with different lags\n",
    "        for lag in range(max_lag_days + 1):\n",
    "            if lag == 0:\n",
    "                temp_shifted = temp_daily\n",
    "                prob_shifted = prob_daily\n",
    "            else:\n",
    "                temp_shifted = temp_daily.shift(lag)\n",
    "                prob_shifted = prob_daily\n",
    "            \n",
    "            # Remove NaN values\n",
    "            combined = pd.concat([temp_shifted, prob_shifted], axis=1, keys=['temp', 'prob']).dropna()\n",
    "            \n",
    "            if len(combined) < 5:\n",
    "                continue\n",
    "            \n",
    "            # Calculate correlation\n",
    "            corr = combined['temp'].corr(combined['prob'])\n",
    "            \n",
    "            if not np.isnan(corr):\n",
    "                correlations.append({\n",
    "                    'market_id': market_id,\n",
    "                    'event_title': market_subset['event_title'].iloc[0],\n",
    "                    'lag_days': lag,\n",
    "                    'correlation': corr,\n",
    "                    'data_points': len(combined),\n",
    "                    'temp_mean': combined['temp'].mean(),\n",
    "                    'prob_mean': combined['prob'].mean()\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(correlations)\n",
    "\n",
    "# Perform correlation analysis\n",
    "correlation_results = analyze_temperature_market_correlation(weather_df, market_df)\n",
    "\n",
    "if correlation_results is not None and not correlation_results.empty:\n",
    "    print(f\"\\nCorrelation Analysis Results:\")\n",
    "    print(f\"Total correlations calculated: {len(correlation_results)}\")\n",
    "    \n",
    "    # Show strongest correlations\n",
    "    print(\"\\nTop 10 strongest correlations:\")\n",
    "    top_corr = correlation_results.nlargest(10, 'correlation')\n",
    "    for _, row in top_corr.iterrows():\n",
    "        print(f\"{row['correlation']:.3f} (lag: {row['lag_days']}d): {row['event_title'][:60]}...\")\n",
    "    \n",
    "    print(\"\\nTop 10 strongest negative correlations:\")\n",
    "    bottom_corr = correlation_results.nsmallest(10, 'correlation')\n",
    "    for _, row in bottom_corr.iterrows():\n",
    "        print(f\"{row['correlation']:.3f} (lag: {row['lag_days']}d): {row['event_title'][:60]}...\")\n",
    "else:\n",
    "    print(\"No correlation results available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlation results\n",
    "if correlation_results is not None and not correlation_results.empty:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Plot 1: Correlation distribution\n",
    "    axes[0, 0].hist(correlation_results['correlation'], bins=20, alpha=0.7, edgecolor='black')\n",
    "    axes[0, 0].set_title('Distribution of Temperature-Market Correlations')\n",
    "    axes[0, 0].set_xlabel('Correlation Coefficient')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Correlation by lag\n",
    "    lag_corr = correlation_results.groupby('lag_days')['correlation'].agg(['mean', 'std', 'count'])\n",
    "    axes[0, 1].errorbar(lag_corr.index, lag_corr['mean'], yerr=lag_corr['std'], \n",
    "                       marker='o', capsize=5, label='Mean ± Std')\n",
    "    axes[0, 1].set_title('Average Correlation by Lag Days')\n",
    "    axes[0, 1].set_xlabel('Lag (Days)')\n",
    "    axes[0, 1].set_ylabel('Average Correlation')\n",
    "    axes[0, 1].axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # Plot 3: Strongest correlations scatter\n",
    "    strong_corr = correlation_results[abs(correlation_results['correlation']) > 0.5]\n",
    "    if not strong_corr.empty:\n",
    "        scatter = axes[1, 0].scatter(strong_corr['temp_mean'], strong_corr['prob_mean'], \n",
    "                                    c=strong_corr['correlation'], cmap='coolwarm', \n",
    "                                    alpha=0.7, s=50)\n",
    "        axes[1, 0].set_title('Strong Correlations: Temperature vs Market Probability')\n",
    "        axes[1, 0].set_xlabel('Average Temperature (°C)')\n",
    "        axes[1, 0].set_ylabel('Average Market Probability')\n",
    "        plt.colorbar(scatter, ax=axes[1, 0], label='Correlation')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Correlation strength vs data points\n",
    "    axes[1, 1].scatter(correlation_results['data_points'], abs(correlation_results['correlation']), \n",
    "                       alpha=0.6, s=30)\n",
    "    axes[1, 1].set_title('Correlation Strength vs Sample Size')\n",
    "    axes[1, 1].set_xlabel('Number of Data Points')\n",
    "    axes[1, 1].set_ylabel('Absolute Correlation')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nCorrelation Summary Statistics:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(correlation_results['correlation'].describe())\n",
    "    \n",
    "    strong_positive = (correlation_results['correlation'] > 0.5).sum()\n",
    "    strong_negative = (correlation_results['correlation'] < -0.5).sum()\n",
    "    \n",
    "    print(f\"\\nStrong correlations (|r| > 0.5):\")\n",
    "    print(f\"Positive: {strong_positive}\")\n",
    "    print(f\"Negative: {strong_negative}\")\n",
    "    print(f\"Total: {strong_positive + strong_negative}\")\n",
    "else:\n",
    "    print(\"No correlation data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Trend Analysis\n",
    "\n",
    "Let's implement more sophisticated trend analysis techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced trend analysis with statistical tests\n",
    "def advanced_trend_analysis(data, name=\"Series\"):\n",
    "    \"\"\"Perform advanced statistical trend analysis\"\"\"\n",
    "    \n",
    "    if data is None or len(data.dropna()) < 20:\n",
    "        print(f\"Insufficient data for advanced analysis of {name}\")\n",
    "        return None\n",
    "    \n",
    "    clean_data = data.dropna()\n",
    "    \n",
    "    results = {\n",
    "        'series_name': name,\n",
    "        'data_points': len(clean_data),\n",
    "        'date_range': f\"{clean_data.index.min()} to {clean_data.index.max()}\",\n",
    "        'mean': clean_data.mean(),\n",
    "        'std': clean_data.std(),\n",
    "        'min': clean_data.min(),\n",
    "        'max': clean_data.max()\n",
    "    }\n",
    "    \n",
    "    # Stationarity tests\n",
    "    try:\n",
    "        # Augmented Dickey-Fuller test\n",
    "        adf_result = adfuller(clean_data)\n",
    "        results['adf_statistic'] = adf_result[0]\n",
    "        results['adf_pvalue'] = adf_result[1]\n",
    "        results['adf_stationary'] = adf_result[1] < 0.05\n",
    "    except:\n",
    "        results['adf_statistic'] = None\n",
    "        results['adf_pvalue'] = None\n",
    "        results['adf_stationary'] = None\n",
    "    \n",
    "    try:\n",
    "        # KPSS test\n",
    "        kpss_result = kpss(clean_data)\n",
    "        results['kpss_statistic'] = kpss_result[0]\n",
    "        results['kpss_pvalue'] = kpss_result[1]\n",
    "        results['kpss_stationary'] = kpss_result[1] >= 0.05\n",
    "    except:\n",
    "        results['kpss_statistic'] = None\n",
    "        results['kpss_pvalue'] = None\n",
    "        results['kpss_stationary'] = None\n",
    "    \n",
    "    # Trend analysis\n",
    "    try:\n",
    "        x = np.arange(len(clean_data))\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(x, clean_data.values)\n",
    "        \n",
    "        results['linear_slope'] = slope\n",
    "        results['linear_intercept'] = intercept\n",
    "        results['linear_r_squared'] = r_value**2\n",
    "        results['linear_p_value'] = p_value\n",
    "        results['linear_trend_significant'] = p_value < 0.05\n",
    "        \n",
    "        # Calculate trend direction and strength\n",
    "        if abs(slope) < 1e-6:\n",
    "            results['trend_direction'] = 'flat'\n",
    "        elif slope > 0:\n",
    "            results['trend_direction'] = 'increasing'\n",
    "        else:\n",
    "            results['trend_direction'] = 'decreasing'\n",
    "            \n",
    "        results['trend_strength'] = abs(slope) * len(clean_data) / clean_data.std()\n",
    "        \n",
    "    except:\n",
    "        results['linear_slope'] = None\n",
    "        results['trend_direction'] = None\n",
    "        results['trend_strength'] = None\n",
    "    \n",
    "    # Volatility analysis\n",
    "    try:\n",
    "        returns = clean_data.pct_change().dropna()\n",
    "        results['volatility'] = returns.std() * np.sqrt(252)  # Annualized volatility\n",
    "        results['skewness'] = stats.skew(clean_data)\n",
    "        results['kurtosis'] = stats.kurtosis(clean_data)\n",
    "    except:\n",
    "        results['volatility'] = None\n",
    "        results['skewness'] = None\n",
    "        results['kurtosis'] = None\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Perform advanced analysis\n",
    "print(\"Performing advanced trend analysis...\")\n",
    "\n",
    "analysis_results = []\n",
    "\n",
    "if weather_df is not None and not weather_df.empty:\n",
    "    temp_analysis = advanced_trend_analysis(weather_df['temperature'], \"Temperature\")\n",
    "    if temp_analysis:\n",
    "        analysis_results.append(temp_analysis)\n",
    "\n",
    "if market_df is not None and not market_df.empty:\n",
    "    # Analyze average probability across all markets\n",
    "    avg_prob = market_df.groupby(market_df.index)['probability'].mean()\n",
    "    prob_analysis = advanced_trend_analysis(avg_prob, \"Market Probability\")\n",
    "    if prob_analysis:\n",
    "        analysis_results.append(prob_analysis)\n",
    "\n",
    "# Display results\n",
    "if analysis_results:\n",
    "    results_df = pd.DataFrame(analysis_results)\n",
    "    print(\"\\nAdvanced Trend Analysis Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for _, row in results_df.iterrows():\n",
    "        print(f\"\\n{row['series_name']}:\")\n",
    "        print(f\"  Data points: {row['data_points']}\")\n",
    "        print(f\"  Date range: {row['date_range']}\")\n",
    "        print(f\"  Mean: {row['mean']:.4f}\")\n",
    "        print(f\"  Trend: {row['trend_direction']} (slope: {row.get('linear_slope', 'N/A')})\")\n",
    "        if row.get('linear_p_value') is not None:\n",
    "            print(f\"  Trend significance: {'Significant' if row['linear_trend_significant'] else 'Not significant'} (p={row['linear_p_value']:.4f})\")\n",
    "        if row.get('volatility') is not None:\n",
    "            print(f\"  Volatility: {row['volatility']:.4f}\")\n",
    "else:\n",
    "    print(\"No data available for advanced analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary and Key Findings\n",
    "\n",
    "This notebook has implemented comprehensive trend detection algorithms for analyzing the relationship between temperature changes and Polymarket probability shifts. Here's what we've accomplished:\n",
    "\n",
    "### Key Features Implemented:\n",
    "1. **Multiple Trend Detection Methods**: Linear regression, moving averages, seasonal decomposition, and change point detection\n",
    "2. **Market Probability Analysis**: Volatility analysis and trend detection for weather-related markets\n",
    "3. **Correlation Analysis**: Statistical correlation between temperature trends and market movements with lag analysis\n",
    "4. **Advanced Statistical Tests**: Stationarity tests (ADF, KPSS) and comprehensive trend analysis\n",
    "5. **Interactive Visualizations**: Multiple plots showing trends, correlations, and patterns\n",
    "\n",
    "### Potential Applications:\n",
    "- **Trading Strategies**: Identify weather-market arbitrage opportunities\n",
    "- **Risk Assessment**: Evaluate market sensitivity to weather patterns\n",
    "- **Predictive Modeling**: Forecast market movements based on weather trends\n",
    "- **Research**: Study climate change impacts on financial markets\n",
    "\n",
    "### Next Steps:\n",
    "1. **Real-time Integration**: Connect with live weather and market data streams\n",
    "2. **Machine Learning**: Implement predictive models using the trend features\n",
    "3. **Geospatial Analysis**: Analyze regional weather-market relationships\n",
    "4. **Portfolio Optimization**: Develop weather-hedged trading strategies\n",
    "\n",
    "### Data Requirements:\n",
    "- Historical weather data (temperature, precipitation, wind, etc.)\n",
    "- Polymarket probability data for weather-related events\n",
    "- Sufficient time overlap between weather and market data\n",
    "- Clean, normalized data in SQLite database\n",
    "\n",
    "This analysis framework provides a solid foundation for understanding and exploiting weather-market correlations in the Polymarket ecosystem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}