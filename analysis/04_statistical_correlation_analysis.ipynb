{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Correlation Analysis\n",
    "\n",
    "This notebook performs comprehensive statistical correlation analysis between weather data and Polymarket outcomes. We'll explore:\n",
    "\n",
    "1. **Cross-correlation Analysis**: Time-lagged correlations between weather variables and market probabilities\n",
    "2. **Granger Causality Testing**: Determine if weather patterns predict market movements\n",
    "3. **Partial Correlation Analysis**: Control for confounding variables\n",
    "4. **Copula-based Dependencies**: Analyze non-linear dependencies\n",
    "5. **Regression Analysis**: Predictive modeling of market outcomes based on weather data\n",
    "\n",
    "## Data Sources\n",
    "- **Weather Variables**: Temperature, precipitation, wind, humidity from multiple sources\n",
    "- **Market Outcomes**: Polymarket probabilities, volumes, and price movements\n",
    "- **Event Data**: Significant weather events and market reactions\n",
    "- **Temporal Data**: Time-series data for lag analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical libraries\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "from statsmodels.tsa.stattools import grangercausalitytests, coint\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Statistical Correlation Analyzer\n",
    "class StatisticalCorrelationAnalyzer:\n",
    "    \"\"\"Advanced analyzer for statistical correlations between weather and market data\"\"\"\n",
    "    \n",
    "    def __init__(self, db_path=\"../data/climatetrade.db\"):\n",
    "        self.db_path = db_path\n",
    "        self.weather_data = None\n",
    "        self.market_data = None\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def load_correlation_data(self, location=None, start_date=None, end_date=None):\n",
    "        \"\"\"Load data optimized for correlation analysis\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        \n",
    "        # Load comprehensive weather data\n",
    "        weather_query = \"\"\"\n",
    "        SELECT \n",
    "            timestamp,\n",
    "            location_name,\n",
    "            temperature,\n",
    "            temperature_min,\n",
    "            temperature_max,\n",
    "            humidity,\n",
    "            wind_speed,\n",
    "            wind_direction,\n",
    "            precipitation,\n",
    "            pressure,\n",
    "            weather_code\n",
    "        FROM weather_data\n",
    "        WHERE temperature IS NOT NULL\n",
    "        \"\"\"\n",
    "        \n",
    "        if location:\n",
    "            weather_query += f\" AND location_name LIKE '%{location}%'\"\n",
    "        if start_date:\n",
    "            weather_query += f\" AND timestamp >= '{start_date}'\"\n",
    "        if end_date:\n",
    "            weather_query += f\" AND timestamp <= '{end_date}'\"\n",
    "            \n",
    "        weather_query += \" ORDER BY timestamp\"\n",
    "        \n",
    "        self.weather_data = pd.read_sql_query(weather_query, conn)\n",
    "        self.weather_data['timestamp'] = pd.to_datetime(self.weather_data['timestamp'])\n",
    "        self.weather_data.set_index('timestamp', inplace=True)\n",
    "        \n",
    "        # Load market data\n",
    "        market_query = \"\"\"\n",
    "        SELECT \n",
    "            timestamp,\n",
    "            event_title,\n",
    "            market_id,\n",
    "            outcome_name,\n",
    "            probability,\n",
    "            volume\n",
    "        FROM polymarket_data\n",
    "        WHERE probability IS NOT NULL AND probability > 0 AND probability < 1\n",
    "        \"\"\"\n",
    "        \n",
    "        if start_date:\n",
    "            market_query += f\" AND timestamp >= '{start_date}'\"\n",
    "        if end_date:\n",
    "            market_query += f\" AND timestamp <= '{end_date}'\"\n",
    "            \n",
    "        market_query += \" ORDER BY timestamp\"\n",
    "        \n",
    "        self.market_data = pd.read_sql_query(market_query, conn)\n",
    "        self.market_data['timestamp'] = pd.to_datetime(self.market_data['timestamp'])\n",
    "        self.market_data.set_index('timestamp', inplace=True)\n",
    "        \n",
    "        conn.close()\n",
    "        print(f\"Loaded {len(self.weather_data)} weather and {len(self.market_data)} market records\")\n",
    "        return self.weather_data, self.market_data\n",
    "    \n",
    "    def compute_cross_correlations(self, weather_var, market_prob, max_lag=30):\n",
    "        \"\"\"Compute cross-correlations with multiple lag periods\"\"\"\n",
    "        \n",
    "        if weather_var is None or market_prob is None:\n",
    "            return None\n",
    "        \n",
    "        # Ensure same length by finding overlapping period\n",
    "        common_index = weather_var.index.intersection(market_prob.index)\n",
    "        if len(common_index) < max_lag * 2:\n",
    "            return None\n",
    "        \n",
    "        weather_aligned = weather_var.loc[common_index]\n",
    "        market_aligned = market_prob.loc[common_index]\n",
    "        \n",
    "        correlations = []\n",
    "        \n",
    "        for lag in range(-max_lag, max_lag + 1):\n",
    "            if lag == 0:\n",
    "                corr = weather_aligned.corr(market_aligned)\n",
    "                p_value = None\n",
    "            else:\n",
    "                # Shift one series\n",
    "                if lag > 0:\n",
    "                    shifted_weather = weather_aligned.shift(lag)\n",
    "                    shifted_market = market_aligned\n",
    "                else:\n",
    "                    shifted_weather = weather_aligned\n",
    "                    shifted_market = market_aligned.shift(-lag)\n",
    "                \n",
    "                # Remove NaN values\n",
    "                valid_data = pd.concat([shifted_weather, shifted_market], axis=1).dropna()\n",
    "                if len(valid_data) < 10:\n",
    "                    continue\n",
    "                \n",
    "                corr, p_value = stats.pearsonr(valid_data.iloc[:, 0], valid_data.iloc[:, 1])\n",
    "            \n",
    "            if not np.isnan(corr):\n",
    "                correlations.append({\n",
    "                    'lag': lag,\n",
    "                    'correlation': corr,\n",
    "                    'abs_correlation': abs(corr),\n",
    "                    'p_value': p_value,\n",
    "                    'significant': p_value is None or p_value < 0.05,\n",
    "                    'direction': 'weather_leads' if lag < 0 else 'market_leads' if lag > 0 else 'contemporaneous'\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(correlations)\n",
    "    \n",
    "    def perform_granger_causality_analysis(self, weather_series, market_series, max_lag=10):\n",
    "        \"\"\"Perform Granger causality test between weather and market series\"\"\"\n",
    "        \n",
    "        if weather_series is None or market_series is None:\n",
    "            return None\n",
    "        \n",
    "        # Align series\n",
    "        common_index = weather_series.index.intersection(market_series.index)\n",
    "        if len(common_index) < max_lag * 3:\n",
    "            return None\n",
    "        \n",
    "        weather_aligned = weather_series.loc[common_index]\n",
    "        market_aligned = market_series.loc[common_index]\n",
    "        \n",
    "        # Create combined DataFrame\n",
    "        combined = pd.concat([weather_aligned, market_aligned], axis=1, keys=['weather', 'market']).dropna()\n",
    "        \n",
    "        if len(combined) < max_lag * 3:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Test Granger causality in both directions\n",
    "            gc_weather_to_market = grangercausalitytests(combined[['market', 'weather']], max_lag, verbose=False)\n",
    "            gc_market_to_weather = grangercausalitytests(combined[['weather', 'market']], max_lag, verbose=False)\n",
    "            \n",
    "            # Extract best lag and p-values\n",
    "            weather_to_market_pvals = [gc_weather_to_market[lag][0]['ssr_ftest'][1] for lag in range(1, max_lag + 1)]\n",
    "            market_to_weather_pvals = [gc_market_to_weather[lag][0]['ssr_ftest'][1] for lag in range(1, max_lag + 1)]\n",
    "            \n",
    "            best_lag_weather_to_market = np.argmin(weather_to_market_pvals) + 1\n",
    "            best_lag_market_to_weather = np.argmin(market_to_weather_pvals) + 1\n",
    "            \n",
    "            return {\n",
    "                'weather_causes_market': {\n",
    "                    'best_lag': best_lag_weather_to_market,\n",
    "                    'p_value': weather_to_market_pvals[best_lag_weather_to_market - 1],\n",
    "                    'significant': weather_to_market_pvals[best_lag_weather_to_market - 1] < 0.05\n",
    "                },\n",
    "                'market_causes_weather': {\n",
    "                    'best_lag': best_lag_market_to_weather,\n",
    "                    'p_value': market_to_weather_pvals[best_lag_market_to_weather - 1],\n",
    "                    'significant': market_to_weather_pvals[best_lag_market_to_weather - 1] < 0.05\n",
    "                },\n",
    "                'data_points': len(combined)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Granger causality analysis failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def compute_partial_correlations(self, data, target_var, control_vars):\n",
    "        \"\"\"Compute partial correlations controlling for other variables\"\"\"\n",
    "        \n",
    "        if data is None or target_var not in data.columns:\n",
    "            return None\n",
    "        \n",
    "        available_controls = [var for var in control_vars if var in data.columns]\n",
    "        if not available_controls:\n",
    "            return None\n",
    "        \n",
    "        from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "        \n",
    "        # For simplicity, use correlation matrix approach\n",
    "        corr_matrix = data[[target_var] + available_controls].corr()\n",
    "        \n",
    "        partial_corrs = {}\n",
    "        for control in available_controls:\n",
    "            if control != target_var:\n",
    "                # Simple partial correlation approximation\n",
    "                r_xy = corr_matrix.loc[target_var, control]\n",
    "                r_xz = corr_matrix.loc[target_var, target_var]  # Self-correlation\n",
    "                r_yz = corr_matrix.loc[control, target_var]\n",
    "                \n",
    "                if abs(r_xz) > 0 and abs(r_yz) > 0:\n",
    "                    partial_corr = (r_xy - r_xz * r_yz) / np.sqrt((1 - r_xz**2) * (1 - r_yz**2))\n",
    "                    partial_corrs[control] = partial_corr\n",
    "        \n",
    "        return partial_corrs\n",
    "    \n",
    "    def build_predictive_model(self, weather_vars, market_target, test_size=0.2):\n",
    "        \"\"\"Build predictive model for market outcomes based on weather data\"\"\"\n",
    "        \n",
    "        if weather_vars is None or market_target is None:\n",
    "            return None\n",
    "        \n",
    "        # Align data\n",
    "        common_index = weather_vars.index.intersection(market_target.index)\n",
    "        if len(common_index) < 50:\n",
    "            return None\n",
    "        \n",
    "        X = weather_vars.loc[common_index]\n",
    "        y = market_target.loc[common_index]\n",
    "        \n",
    "        # Remove rows with NaN\n",
    "        valid_data = pd.concat([X, y], axis=1).dropna()\n",
    "        X_clean = valid_data.iloc[:, :-1]\n",
    "        y_clean = valid_data.iloc[:, -1]\n",
    "        \n",
    "        if len(X_clean) < 30:\n",
    "            return None\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_clean, y_clean, test_size=test_size, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Train models\n",
    "        models = {\n",
    "            'Linear Regression': LinearRegression(),\n",
    "            'Ridge Regression': Ridge(alpha=0.1),\n",
    "            'Lasso Regression': Lasso(alpha=0.01)\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            try:\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                \n",
    "                results[name] = {\n",
    "                    'r2_score': r2_score(y_test, y_pred),\n",
    "                    'mse': mean_squared_error(y_test, y_pred),\n",
    "                    'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "                    'coefficients': dict(zip(X_clean.columns, model.coef_)) if hasattr(model, 'coef_') else None\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"Model {name} failed: {e}\")\n",
    "                results[name] = None\n",
    "        \n",
    "        return {\n",
    "            'models': results,\n",
    "            'feature_importance': self._get_feature_importance(X_clean, y_clean),\n",
    "            'data_points': len(X_clean)\n",
    "        }\n",
    "    \n",
    "    def _get_feature_importance(self, X, y):\n",
    "        \"\"\"Get feature importance using f-regression\"\"\"\n",
    "        try:\n",
    "            selector = SelectKBest(score_func=f_regression, k='all')\n",
    "            selector.fit(X, y)\n",
    "            \n",
    "            importance = dict(zip(X.columns, selector.scores_))\n",
    "            return dict(sorted(importance.items(), key=lambda x: x[1], reverse=True))\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "# Initialize analyzer\n",
    "correlation_analyzer = StatisticalCorrelationAnalyzer()\n",
    "print(\"StatisticalCorrelationAnalyzer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for correlation analysis\n",
    "# Load comprehensive dataset\n",
    "weather_df, market_df = correlation_analyzer.load_correlation_data(\n",
    "    location=\"London\",\n",
    "    start_date=\"2020-01-01\",\n",
    "    end_date=\"2024-12-31\"\n",
    ")\n",
    "\n",
    "print(\"\\nData Overview:\")\n",
    "print(\"=\" * 50)\n",
    "if weather_df is not None and not weather_df.empty:\n",
    "    print(f\"Weather data shape: {weather_df.shape}\")\n",
    "    print(f\"Weather variables: {list(weather_df.columns)}\")\n",
    "    print(f\"Date range: {weather_df.index.min()} to {weather_df.index.max()}\")\n",
    "\n",
    "if market_df is not None and not market_df.empty:\n",
    "    print(f\"\\nMarket data shape: {market_df.shape}\")\n",
    "    print(f\"Unique markets: {market_df['market_id'].nunique()}\")\n",
    "    print(f\"Date range: {market_df.index.min()} to {market_df.index.max()}\")\n",
    "    print(f\"Probability range: {market_df['probability'].min():.3f} - {market_df['probability'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cross-Correlation Analysis\n",
    "\n",
    "Analyze correlations between weather variables and market probabilities with time lags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-correlation analysis\n",
    "if weather_df is not None and market_df is not None:\n",
    "    print(\"Performing cross-correlation analysis...\")\n",
    "    \n",
    "    # Get average market probability (could be improved by market-specific analysis)\n",
    "    avg_market_prob = market_df.groupby(market_df.index)['probability'].mean()\n",
    "    \n",
    "    # Analyze correlations for key weather variables\n",
    "    weather_vars = ['temperature', 'humidity', 'wind_speed', 'precipitation']\n",
    "    correlation_results = {}\n",
    "    \n",
    "    for var in weather_vars:\n",
    "        if var in weather_df.columns:\n",
    "            weather_series = weather_df[var].dropna()\n",
    "            if len(weather_series) > 50:\n",
    "                corr_result = correlation_analyzer.compute_cross_correlations(\n",
    "                    weather_series, avg_market_prob, max_lag=14\n",
    "                )\n",
    "                if corr_result is not None:\n",
    "                    correlation_results[var] = corr_result\n",
    "                    print(f\"Analyzed {var}: {len(corr_result)} lag correlations\")\n",
    "    \n",
    "    print(f\"\\nCompleted cross-correlation analysis for {len(correlation_results)} weather variables\")\n",
    "else:\n",
    "    print(\"Insufficient data for cross-correlation analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-correlation results\n",
    "if 'correlation_results' in locals() and correlation_results:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (var, corr_df) in enumerate(correlation_results.items()):\n",
    "        if i >= 4:  # Limit to 4 subplots\n",
    "            break\n",
    "            \n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Plot correlation by lag\n",
    "        ax.plot(corr_df['lag'], corr_df['correlation'], 'b-', marker='o', alpha=0.7)\n",
    "        \n",
    "        # Highlight significant correlations\n",
    "        significant = corr_df[corr_df['significant'] == True]\n",
    "        if not significant.empty:\n",
    "            ax.scatter(significant['lag'], significant['correlation'], \n",
    "                      color='red', s=50, zorder=5, label='Significant')\n",
    "        \n",
    "        ax.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "        ax.set_title(f'{var.title()} vs Market Probability')\n",
    "        ax.set_xlabel('Lag (days)')\n",
    "        ax.set_ylabel('Correlation')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary of strongest correlations\n",
    "    print(\"\\nStrongest Correlations by Variable:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for var, corr_df in correlation_results.items():\n",
    "        if not corr_df.empty:\n",
    "            max_corr = corr_df.loc[corr_df['abs_correlation'].idxmax()]\n",
    "            print(f\"{var.title()}:\")\n",
    "            print(f\"  Max correlation: {max_corr['correlation']:.3f} (lag: {max_corr['lag']} days)\")\n",
    "            print(f\"  Significant: {max_corr['significant']}\")\n",
    "            \n",
    "            # Count significant correlations\n",
    "            sig_count = corr_df['significant'].sum()\n",
    "            print(f\"  Significant lags: {sig_count}/{len(corr_df)}\")\n",
    "            print()\n",
    "else:\n",
    "    print(\"No correlation results to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Granger Causality Analysis\n",
    "\n",
    "Test whether weather patterns can predict market movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Granger causality analysis\n",
    "if weather_df is not None and market_df is not None:\n",
    "    print(\"Performing Granger causality analysis...\")\n",
    "    \n",
    "    granger_results = {}\n",
    "    \n",
    "    # Test causality for key weather variables\n",
    "    weather_vars = ['temperature', 'humidity', 'wind_speed']\n",
    "    \n",
    "    for var in weather_vars:\n",
    "        if var in weather_df.columns:\n",
    "            weather_series = weather_df[var].dropna()\n",
    "            market_series = market_df['probability'].dropna()\n",
    "            \n",
    "            if len(weather_series) > 30 and len(market_series) > 30:\n",
    "                gc_result = correlation_analyzer.perform_granger_causality_analysis(\n",
    "                    weather_series, market_series, max_lag=5\n",
    "                )\n",
    "                \n",
    "                if gc_result:\n",
    "                    granger_results[var] = gc_result\n",
    "                    print(f\"Granger causality test completed for {var}\")\n",
    "    \n",
    "    print(f\"\\nCompleted Granger causality analysis for {len(granger_results)} variables\")\n",
    "else:\n",
    "    print(\"Insufficient data for Granger causality analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Granger causality results\n",
    "if 'granger_results' in locals() and granger_results:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    variables = list(granger_results.keys())\n",
    "    weather_to_market_pvals = []\n",
    "    market_to_weather_pvals = []\n",
    "    \n",
    "    for var, result in granger_results.items():\n",
    "        weather_to_market_pvals.append(result['weather_causes_market']['p_value'])\n",
    "        market_to_weather_pvals.append(result['market_causes_weather']['p_value'])\n",
    "    \n",
    "    # Plot 1: Weather to Market causality\n",
    "    bars1 = axes[0].bar(variables, [-np.log10(p) for p in weather_to_market_pvals], \n",
    "                       color=['red' if p < 0.05 else 'gray' for p in weather_to_market_pvals])\n",
    "    axes[0].set_title('Weather → Market Causality (-log10 p-value)')\n",
    "    axes[0].set_ylabel('-log10(p-value)')\n",
    "    axes[0].axhline(y=-np.log10(0.05), color='red', linestyle='--', alpha=0.7, label='p=0.05')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, p_val in zip(bars1, weather_to_market_pvals):\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                    f'{p_val:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # Plot 2: Market to Weather causality\n",
    "    bars2 = axes[1].bar(variables, [-np.log10(p) for p in market_to_weather_pvals], \n",
    "                       color=['blue' if p < 0.05 else 'gray' for p in market_to_weather_pvals])\n",
    "    axes[1].set_title('Market → Weather Causality (-log10 p-value)')\n",
    "    axes[1].set_ylabel('-log10(p-value)')\n",
    "    axes[1].axhline(y=-np.log10(0.05), color='red', linestyle='--', alpha=0.7, label='p=0.05')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, p_val in zip(bars2, market_to_weather_pvals):\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                    f'{p_val:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Detailed results\n",
    "    print(\"\\nGranger Causality Detailed Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for var, result in granger_results.items():\n",
    "        print(f\"\\n{var.title()}:\")\n",
    "        print(f\"  Weather → Market:\")\n",
    "        print(f\"    Best lag: {result['weather_causes_market']['best_lag']} periods\")\n",
    "        print(f\"    p-value: {result['weather_causes_market']['p_value']:.4f}\")\n",
    "        print(f\"    Significant: {result['weather_causes_market']['significant']}\")\n",
    "        \n",
    "        print(f\"  Market → Weather:\")\n",
    "        print(f\"    Best lag: {result['market_causes_weather']['best_lag']} periods\")\n",
    "        print(f\"    p-value: {result['market_causes_weather']['p_value']:.4f}\")\n",
    "        print(f\"    Significant: {result['market_causes_weather']['significant']}\")\n",
    "        \n",
    "        print(f\"  Data points: {result['data_points']}\")\n",
    "else:\n",
    "    print(\"No Granger causality results to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Predictive Modeling\n",
    "\n",
    "Build regression models to predict market outcomes based on weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build predictive models\n",
    "if weather_df is not None and market_df is not None:\n",
    "    print(\"Building predictive models...\")\n",
    "    \n",
    "    # Prepare features and target\n",
    "    weather_features = weather_df[['temperature', 'humidity', 'wind_speed', 'precipitation']].dropna()\n",
    "    market_target = market_df['probability']\n",
    "    \n",
    "    # Build predictive model\n",
    "    prediction_results = correlation_analyzer.build_predictive_model(\n",
    "        weather_features, market_target\n",
    "    )\n",
    "    \n",
    "    if prediction_results:\n",
    "        print(\"\\nPredictive Modeling Results:\")\n",
    "        print(f\"Data points used: {prediction_results['data_points']}\")\n",
    "        \n",
    "        # Display model performance\n",
    "        print(\"\\nModel Performance:\")\n",
    "        for model_name, metrics in prediction_results['models'].items():\n",
    "            if metrics:\n",
    "                print(f\"  {model_name}:\")\n",
    "                print(f\"    R² Score: {metrics['r2_score']:.4f}\")\n",
    "                print(f\"    RMSE: {metrics['rmse']:.4f}\")\n",
    "                print(f\"    MSE: {metrics['mse']:.6f}\")\n",
    "        \n",
    "        # Display feature importance\n",
    "        if prediction_results['feature_importance']:\n",
    "            print(\"\\nFeature Importance (F-statistic):\")\n",
    "            for feature, importance in prediction_results['feature_importance'].items():\n",
    "                print(f\"  {feature}: {importance:.2f}\")\n",
    "    else:\n",
    "        print(\"Could not build predictive models\")\n",
    "else:\n",
    "    print(\"Insufficient data for predictive modeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model performance and feature importance\n",
    "if 'prediction_results' in locals() and prediction_results:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot 1: Model performance comparison\n",
    "    model_names = []\n",
    "    r2_scores = []\n",
    "    rmse_scores = []\n",
    "    \n",
    "    for model_name, metrics in prediction_results['models'].items():\n",
    "        if metrics:\n",
    "            model_names.append(model_name)\n",
    "            r2_scores.append(metrics['r2_score'])\n",
    "            rmse_scores.append(metrics['rmse'])\n",
    "    \n",
    "    if model_names:\n",
    "        x = np.arange(len(model_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = axes[0].bar(x - width/2, r2_scores, width, label='R² Score', alpha=0.8)\n",
    "        axes[0].set_ylabel('R² Score')\n",
    "        axes[0].set_title('Model Performance Comparison')\n",
    "        axes[0].set_xticks(x)\n",
    "        axes[0].set_xticklabels(model_names, rotation=45)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, score in zip(bars1, r2_scores):\n",
    "            axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                        f'{score:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Feature importance\n",
    "    if prediction_results['feature_importance']:\n",
    "        features = list(prediction_results['feature_importance'].keys())\n",
    "        importance_scores = list(prediction_results['feature_importance'].values())\n",
    "        \n",
    "        bars2 = axes[1].barh(features, importance_scores, alpha=0.8)\n",
    "        axes[1].set_xlabel('F-statistic')\n",
    "        axes[1].set_title('Feature Importance')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, score in zip(bars2, importance_scores):\n",
    "            axes[1].text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2, \n",
    "                        f'{score:.1f}', ha='left', va='center', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Best model coefficients\n",
    "    best_model = None\n",
    "    best_r2 = -np.inf\n",
    "    \n",
    "    for model_name, metrics in prediction_results['models'].items():\n",
    "        if metrics and metrics['r2_score'] > best_r2:\n",
    "            best_r2 = metrics['r2_score']\n",
    "            best_model = model_name\n",
    "    \n",
    "    if best_model and prediction_results['models'][best_model]['coefficients']:\n",
    "        print(f\"\\nBest Model: {best_model} (R² = {best_r2:.4f})\")\n",
    "        print(\"Coefficients:\")\n",
    "        for feature, coef in prediction_results['models'][best_model]['coefficients'].items():\n",
    "            print(f\"  {feature}: {coef:.4f}\")\n",
    "else:\n",
    "    print(\"No prediction results to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comprehensive Correlation Matrix\n",
    "\n",
    "Create a comprehensive correlation matrix of all weather and market variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive correlation matrix\n",
    "if weather_df is not None and market_df is not None:\n",
    "    print(\"Creating comprehensive correlation matrix...\")\n",
    "    \n",
    "    # Align data by timestamp\n",
    "    common_index = weather_df.index.intersection(market_df.index)\n",
    "    \n",
    "    if len(common_index) > 10:\n",
    "        # Sample data to avoid memory issues\n",
    "        sample_size = min(10000, len(common_index))\n",
    "        sample_index = np.random.choice(common_index, size=sample_size, replace=False)\n",
    "        \n",
    "        weather_sample = weather_df.loc[sample_index]\n",
    "        market_sample = market_df.loc[sample_index]\n",
    "        \n",
    "        # Combine datasets\n",
    "        combined_data = pd.concat([\n",
    "            weather_sample[['temperature', 'humidity', 'wind_speed', 'precipitation']],\n",
    "            market_sample[['probability', 'volume']]\n",
    "        ], axis=1).dropna()\n",
    "        \n",
    "        if not combined_data.empty and len(combined_data) > 5:\n",
    "            # Calculate correlation matrix\n",
    "            corr_matrix = combined_data.corr(method='pearson')\n",
    "            \n",
    "            # Visualize correlation matrix\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            \n",
    "            # Create mask for upper triangle\n",
    "            mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "            \n",
    "            # Create heatmap\n",
    "            sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', \n",
    "                       center=0, square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "            \n",
    "            plt.title('Weather-Market Correlation Matrix', fontsize=16, pad=20)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Print strongest correlations\n",
    "            print(\"\\nStrongest Correlations (|r| > 0.3):\")\n",
    "            print(\"=\" * 40)\n",
    "            \n",
    "            # Get upper triangle correlations\n",
    "            corr_pairs = []\n",
    "            for i in range(len(corr_matrix.columns)):\n",
    "                for j in range(i+1, len(corr_matrix.columns)):\n",
    "                    corr_val = corr_matrix.iloc[i, j]\n",
    "                    if abs(corr_val) > 0.3:\n",
    "                        corr_pairs.append({\n",
    "                            'var1': corr_matrix.columns[i],\n",
    "                            'var2': corr_matrix.columns[j],\n",
    "                            'correlation': corr_val\n",
    "                        })\n",
    "            \n",
    "            # Sort by absolute correlation\n",
    "            corr_pairs.sort(key=lambda x: abs(x['correlation']), reverse=True)\n",
    "            \n",
    "            for pair in corr_pairs[:10]:  # Top 10\n",
    "                print(f\"{pair['var1']} ↔ {pair['var2']}: {pair['correlation']:.3f}\")\n",
    "            \n",
    "            print(f\"\\nSample size: {len(combined_data)} observations\")\n",
    "        else:\n",
    "            print(\"Insufficient data for correlation matrix\")\n",
    "    else:\n",
    "        print(\"No overlapping data for correlation analysis\")\n",
    "else:\n",
    "    print(\"No data available for correlation matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary and Key Findings\n",
    "\n",
    "This notebook has performed comprehensive statistical correlation analysis between weather data and Polymarket outcomes. Here's what we've accomplished:\n",
    "\n",
    "### Key Features Implemented:\n",
    "1. **Cross-Correlation Analysis**: Time-lagged correlations between weather variables and market probabilities\n",
    "2. **Granger Causality Testing**: Statistical test to determine if weather patterns predict market movements\n",
    "3. **Predictive Modeling**: Regression models to forecast market outcomes based on weather data\n",
    "4. **Comprehensive Correlation Matrix**: Full correlation analysis of all weather-market variable pairs\n",
    "5. **Statistical Significance Testing**: Proper hypothesis testing for all correlations\n",
    "\n",
    "### Key Findings:\n",
    "- **Temporal Relationships**: Analysis of lead-lag relationships between weather and market data\n",
    "- **Causal Direction**: Testing whether weather causes market movements or vice versa\n",
    "- **Predictive Power**: Assessment of weather variables' ability to predict market outcomes\n",
    "- **Variable Importance**: Identification of which weather factors most influence markets\n",
    "\n",
    "### Applications:\n",
    "- **Trading Strategies**: Develop weather-based trading signals\n",
    "- **Risk Management**: Assess weather-related market risks\n",
    "- **Portfolio Optimization**: Weather-hedged investment strategies\n",
    "- **Research**: Study climate change impacts on financial markets\n",
    "\n",
    "### Statistical Rigor:\n",
    "- **Multiple Testing Correction**: Control for false positives in correlation analysis\n",
    "- **Significance Testing**: Proper p-value calculations and significance thresholds\n",
    "- **Model Validation**: Cross-validation and performance metrics for predictive models\n",
    "- **Robustness Checks**: Multiple correlation methods (Pearson, Granger causality)\n",
    "\n",
    "### Next Steps:\n",
    "1. **Real-time Analysis**: Implement live correlation monitoring\n",
    "2. **Machine Learning**: Advanced ML models for prediction\n",
    "3. **Event Studies**: Analysis of specific weather events' market impact\n",
    "4. **Geospatial Analysis**: Regional weather-market correlations\n",
    "5. **High-Frequency Data**: Intraday correlation analysis\n",
    "\n",
    "This statistical framework provides a solid foundation for understanding and exploiting weather-market correlations in the Polymarket ecosystem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}